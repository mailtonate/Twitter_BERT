{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a90773ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: textblob in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (0.17.1)\n",
      "Requirement already satisfied: nltk>=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from textblob) (3.4.4)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nltk>=3.1->textblob) (1.15.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: tweepy in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (4.6.0)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tweepy) (3.2.0)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tweepy) (2.27.1)\n",
      "Requirement already satisfied: requests-oauthlib<2,>=1.2.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from tweepy) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.27.0->tweepy) (2021.5.30)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.27.0->tweepy) (2.0.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.27.0->tweepy) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.27.0->tweepy) (3.1)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: pycountry in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (22.1.10)\n",
      "Requirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pycountry) (49.6.0.post20210108)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: wordcloud in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.8.1)\n",
      "Requirement already satisfied: matplotlib in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from wordcloud) (3.3.4)\n",
      "Requirement already satisfied: pillow in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from wordcloud) (8.4.0)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from wordcloud) (1.19.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.15.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Requirement already satisfied: langdetect in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (1.0.9)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from langdetect) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "# Install Libraries - only need to do this once\n",
    "!pip install textblob\n",
    "!pip install tweepy\n",
    "!pip install pycountry\n",
    "!pip install wordcloud\n",
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5be659",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference for analysis template\n",
    "#https://towardsdatascience.com/step-by-step-twitter-sentiment-analysis-in-python-d6f650ade58d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed9276a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from textblob import TextBlob\n",
    "import sys\n",
    "import tweepy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import pycountry\n",
    "import re\n",
    "import string\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from langdetect import detect\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bdd3af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter Authentication\n",
    "#call stored variables\n",
    "%store -r consumerKey\n",
    "consumerKey\n",
    "%store -r consumerSecret\n",
    "consumerSecret\n",
    "%store -r accessToken\n",
    "accessToken\n",
    "%store -r accessTokenSecret\n",
    "accessTokenSecret\n",
    "auth = tweepy.OAuthHandler(consumerKey, consumerSecret)\n",
    "auth.set_access_token(accessToken, accessTokenSecret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155347b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment Analysis\n",
    "def percentage(part,whole):\n",
    "    return 100 * float(part)/float(whole)\n",
    "keyword = input(\"Please enter keyword to search: \")#Tesla\n",
    "noOfTweet = int(input (\"Please enter number of tweets to analyze: \"))#1000\n",
    "tweets = tweepy.Cursor(api.search_tweets, q=keyword).items(noOfTweet)\n",
    "positive = 0\n",
    "negative = 0\n",
    "neutral = 0\n",
    "polarity = 0\n",
    "tweet_list = []\n",
    "neutral_list = []\n",
    "negative_list = []\n",
    "positive_list = []\n",
    "for tweet in tweets:\n",
    " \n",
    " #print(tweet.text)\n",
    "    tweet_list.append(tweet.text)\n",
    "    analysis = TextBlob(tweet.text)\n",
    "    score = SentimentIntensityAnalyzer().polarity_scores(tweet.text)\n",
    "    neg = score['neg']\n",
    "    neu = score['neu']\n",
    "    pos = score['pos']\n",
    "    comp = score['compound']\n",
    "    polarity += analysis.sentiment.polarity\n",
    "\n",
    "if neg > pos:\n",
    "    negative_list.append(tweet.text)\n",
    "    negative += 1\n",
    "elif pos > neg:\n",
    "    positive_list.append(tweet.text)\n",
    "    positive += 1\n",
    "elif pos == neg:\n",
    "    neutral_list.append(tweet.text)\n",
    "    neutral += 1\n",
    "positive = percentage(positive, noOfTweet)\n",
    "negative = percentage(negative, noOfTweet)\n",
    "neutral = percentage(neutral, noOfTweet)\n",
    "polarity = percentage(polarity, noOfTweet)\n",
    "positive = format(positive, '.1f')\n",
    "negative = format(negative, '.1f')\n",
    "neutral = format(neutral, '.1f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae123a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Tweets (Total, Positive, Negative, Neutral)\n",
    "tweet_list = pd.DataFrame(tweet_list)\n",
    "neutral_list = pd.DataFrame(neutral_list)\n",
    "negative_list = pd.DataFrame(negative_list)\n",
    "positive_list = pd.DataFrame(positive_list)\n",
    "print(“total number: “,len(tweet_list))\n",
    "print(“positive number: “,len(positive_list))\n",
    "print(“negative number: “, len(negative_list))\n",
    "print(“neutral number: “,len(neutral_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfaf4744",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start of Vader model\n",
    "\n",
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "finviz_url = 'https://finviz.com/quote.ashx?t='\n",
    "tickers = ['AAPL']\n",
    "\n",
    "news_tables = {}\n",
    "for ticker in tickers:\n",
    "    url = finviz_url + ticker\n",
    "\n",
    "    req = Request(url=url, headers={'user-agent': 'my-app'})\n",
    "    response = urlopen(req)\n",
    "\n",
    "    html = BeautifulSoup(response, features='html.parser')\n",
    "    news_table = html.find(id='news-table')\n",
    "    news_tables[ticker] = news_table\n",
    "\n",
    "parsed_data = []\n",
    "\n",
    "for ticker, news_table in news_tables.items():\n",
    "\n",
    "    for row in news_table.findAll('tr'):\n",
    "\n",
    "        title = row.a.text\n",
    "        date_data = row.td.text.split(' ')\n",
    "\n",
    "        if len(date_data) == 1:\n",
    "            time = date_data[0]\n",
    "        else:\n",
    "            date = date_data[0]\n",
    "            time = date_data[1]\n",
    "\n",
    "        parsed_data.append([ticker, date, time, title])\n",
    "\n",
    "df = pd.DataFrame(parsed_data, columns=['ticker', 'date', 'time', 'title'])\n",
    "\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "f = lambda title: vader.polarity_scores(title)['compound']\n",
    "df['compound'] = df['title'].apply(f)\n",
    "df['date'] = pd.to_datetime(df.date).dt.date\n",
    "\n",
    "\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "f = lambda title: vader.polarity_scores(title)['compound']\n",
    "df['compound'] = df['title'].apply(f)\n",
    "df['date'] = pd.to_datetime(df.date).dt.date\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015e3fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
