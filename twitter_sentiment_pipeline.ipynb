{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a8c864d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting textblob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "     |████████████████████████████████| 636 kB 8.3 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: nltk>=3.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from textblob) (3.4.4)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from nltk>=3.1->textblob) (1.15.0)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.17.1\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting tweepy\n",
      "  Downloading tweepy-4.5.0-py2.py3-none-any.whl (66 kB)\n",
      "     |████████████████████████████████| 66 kB 955 kB/s             \n",
      "\u001b[?25hCollecting requests<3,>=2.27.0\n",
      "  Using cached requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
      "Collecting requests-oauthlib<2,>=1.0.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.27.0->tweepy) (2.0.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.27.0->tweepy) (3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.27.0->tweepy) (1.26.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from requests<3,>=2.27.0->tweepy) (2021.5.30)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Downloading oauthlib-3.2.0-py3-none-any.whl (151 kB)\n",
      "     |████████████████████████████████| 151 kB 15.7 MB/s            \n",
      "\u001b[?25hInstalling collected packages: requests, oauthlib, requests-oauthlib, tweepy\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.26.0\n",
      "    Uninstalling requests-2.26.0:\n",
      "      Successfully uninstalled requests-2.26.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "anaconda-project 0.9.1 requires ruamel-yaml, which is not installed.\u001b[0m\n",
      "Successfully installed oauthlib-3.2.0 requests-2.27.1 requests-oauthlib-1.3.1 tweepy-4.5.0\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting pycountry\n",
      "  Downloading pycountry-22.1.10.tar.gz (10.1 MB)\n",
      "     |████████████████████████████████| 10.1 MB 6.2 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from pycountry) (49.6.0.post20210108)\n",
      "Building wheels for collected packages: pycountry\n",
      "  Building wheel for pycountry (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pycountry: filename=pycountry-22.1.10-py2.py3-none-any.whl size=10595765 sha256=bc2ca127ce078f5c5542b703c0108793672a02fd158dc8cfa558ff77f848a6b0\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/dc/59/d3/291ec580b43cff4f143ee86ee7ff1100cc988f6972a3133c10\n",
      "Successfully built pycountry\n",
      "Installing collected packages: pycountry\n",
      "Successfully installed pycountry-22.1.10\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.8.1-cp36-cp36m-manylinux1_x86_64.whl (366 kB)\n",
      "     |████████████████████████████████| 366 kB 8.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.6.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from wordcloud) (1.19.5)\n",
      "Requirement already satisfied: matplotlib in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from wordcloud) (3.3.4)\n",
      "Requirement already satisfied: pillow in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from wordcloud) (8.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.15.0)\n",
      "Installing collected packages: wordcloud\n",
      "Successfully installed wordcloud-1.8.1\n",
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     |████████████████████████████████| 981 kB 6.4 MB/s            \n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages (from langdetect) (1.15.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993221 sha256=53c0bc2bb5b96ecd7f0ce83e6aac1677cb975e77b852dea374749cebc6b88f9d\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/22/e8/62/ef79403841bab16f1c4260b967bee7fa579d78552a66c7f6e0\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "# Install Libraries - only need to do this once\n",
    "!pip install textblob\n",
    "!pip install tweepy\n",
    "!pip install pycountry\n",
    "!pip install wordcloud\n",
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa837ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reference for analysis template\n",
    "#https://towardsdatascience.com/step-by-step-twitter-sentiment-analysis-in-python-d6f650ade58d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ff87b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "# Import Libraries\n",
    "from textblob import TextBlob\n",
    "import sys\n",
    "import tweepy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import nltk\n",
    "import pycountry\n",
    "import re\n",
    "import string\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from langdetect import detect\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "509b7d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Twitter Authentication\n",
    "consumerKey = “Type your consumer key here”\n",
    "consumerSecret = “Type your consumer secret here”\n",
    "accessToken = “Type your accedd token here”\n",
    "accessTokenSecret = “Type your access token secret here”\n",
    "auth = tweepy.OAuthHandler(consumerKey, consumerSecret)\n",
    "auth.set_access_token(accessToken, accessTokenSecret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d2b062",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sentiment Analysis\n",
    "def percentage(part,whole):\n",
    " return 100 * float(part)/float(whole)\n",
    "keyword = input(“Please enter keyword or hashtag to search: “)#Tesla\n",
    "noOfTweet = int(input (“Please enter how many tweets to analyze: “))#1000\n",
    "tweets = tweepy.Cursor(api.search, q=keyword).items(noOfTweet)\n",
    "positive = 0\n",
    "negative = 0\n",
    "neutral = 0\n",
    "polarity = 0\n",
    "tweet_list = []\n",
    "neutral_list = []\n",
    "negative_list = []\n",
    "positive_list = []\n",
    "for tweet in tweets:\n",
    " \n",
    " #print(tweet.text)\n",
    " tweet_list.append(tweet.text)\n",
    " analysis = TextBlob(tweet.text)\n",
    " score = SentimentIntensityAnalyzer().polarity_scores(tweet.text)\n",
    " neg = score[‘neg’]\n",
    " neu = score[‘neu’]\n",
    " pos = score[‘pos’]\n",
    " comp = score[‘compound’]\n",
    " polarity += analysis.sentiment.polarity\n",
    " \n",
    " if neg > pos:\n",
    " negative_list.append(tweet.text)\n",
    " negative += 1\n",
    "elif pos > neg:\n",
    " positive_list.append(tweet.text)\n",
    " positive += 1\n",
    " \n",
    " elif pos == neg:\n",
    " neutral_list.append(tweet.text)\n",
    " neutral += 1\n",
    "positive = percentage(positive, noOfTweet)\n",
    "negative = percentage(negative, noOfTweet)\n",
    "neutral = percentage(neutral, noOfTweet)\n",
    "polarity = percentage(polarity, noOfTweet)\n",
    "positive = format(positive, ‘.1f’)\n",
    "negative = format(negative, ‘.1f’)\n",
    "neutral = format(neutral, ‘.1f’)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8814995",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Number of Tweets (Total, Positive, Negative, Neutral)\n",
    "tweet_list = pd.DataFrame(tweet_list)\n",
    "neutral_list = pd.DataFrame(neutral_list)\n",
    "negative_list = pd.DataFrame(negative_list)\n",
    "positive_list = pd.DataFrame(positive_list)\n",
    "print(“total number: “,len(tweet_list))\n",
    "print(“positive number: “,len(positive_list))\n",
    "print(“negative number: “, len(negative_list))\n",
    "print(“neutral number: “,len(neutral_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13e0e37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start of Vader model\n",
    "\n",
    "from urllib.request import urlopen, Request\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "finviz_url = 'https://finviz.com/quote.ashx?t='\n",
    "tickers = ['AAPL']\n",
    "\n",
    "news_tables = {}\n",
    "for ticker in tickers:\n",
    "    url = finviz_url + ticker\n",
    "\n",
    "    req = Request(url=url, headers={'user-agent': 'my-app'})\n",
    "    response = urlopen(req)\n",
    "\n",
    "    html = BeautifulSoup(response, features='html.parser')\n",
    "    news_table = html.find(id='news-table')\n",
    "    news_tables[ticker] = news_table\n",
    "\n",
    "parsed_data = []\n",
    "\n",
    "for ticker, news_table in news_tables.items():\n",
    "\n",
    "    for row in news_table.findAll('tr'):\n",
    "\n",
    "        title = row.a.text\n",
    "        date_data = row.td.text.split(' ')\n",
    "\n",
    "        if len(date_data) == 1:\n",
    "            time = date_data[0]\n",
    "        else:\n",
    "            date = date_data[0]\n",
    "            time = date_data[1]\n",
    "\n",
    "        parsed_data.append([ticker, date, time, title])\n",
    "\n",
    "df = pd.DataFrame(parsed_data, columns=['ticker', 'date', 'time', 'title'])\n",
    "\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "f = lambda title: vader.polarity_scores(title)['compound']\n",
    "df['compound'] = df['title'].apply(f)\n",
    "df['date'] = pd.to_datetime(df.date).dt.date\n",
    "\n",
    "\n",
    "vader = SentimentIntensityAnalyzer()\n",
    "\n",
    "f = lambda title: vader.polarity_scores(title)['compound']\n",
    "df['compound'] = df['title'].apply(f)\n",
    "df['date'] = pd.to_datetime(df.date).dt.date\n",
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2b22b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
